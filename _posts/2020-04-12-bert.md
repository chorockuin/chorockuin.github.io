---
layout: post
title: "BERT(Bidirectional Encoder Representations from Transformers)"
---
## 원문 
- <https://arxiv.org/abs/1810.04805>

## Abstract
- We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.
	- 트랜스포머 기반의 양방향 인코더
-  BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.
	- 레이블링되지 않은 텍스트를 양방향으로 사전 학습했다.
- As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. 
	- 하나의 출력 레이어만 추가하여 finetuning 하면, 아키텍쳐 변경 없이 다양한 NLP 모델을 만들 수 있다.
- BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
	- 11개의 NLP 작업에서 state-of-the-art 달성했다.

## Introduction
- Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference (Bowman et al., 2015; Williams et al., 2018) and paraphrasing (Dolan and Brockett, 2005), which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).
	- 언어 모델의 사전 학습은 문장 처리 task와 토큰 처리 task에 모두 효과적임이 증명되었다.
- There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.
	- 현재 사전 학습된 언어 표현을 적용하는 방식에는 feature-based와 fine-tuning 2가지가 있다.
	- feature-based 방식은 원하는 task에 맞게 따로 아키텍쳐를 만든다. 이 아키텍쳐에 사전 학습된 표현들을 이용한다. ELMo 등이 이런 방식.
	- fine-tuning 방식은 원하는 task에 맞는 파라미터만 선택하고 아키텍쳐는 변경하지 않는다. downstream task에서 학습하면서 선택한 파라미터 값들을 미세 조정한다. OpenAI GPT 등이 이런 방식.
- The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the fine-tuning approaches. For example, in OpenAI GPT, the authors use a left-to right architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer (Vaswani et al., 2017).
	- 이 두가지 방식은 모두 단방향으로만 텍스트를 학습하는데, 이는 사전 학습 방식의 강력함을 제한한다. 특히 fine-tuning에서는 더 그렇다.
	- OpenAI GPT 예를 들면, 트랜스포머의 셀프 어텐션 레이어에서 앞에 출현하는 토큰들은 뒤에 출현하는 토큰들에 대한 어텐션 값을 가지지 못한다. [Transformers 디코더의 셀프 어텐션 참조](http://chorockuin.github.io/2020/04/11/transformers.html)
- Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions.
	- 이는 문장 처리 task들에 좋지 않고, 양방향 문맥 통합이 중요한 토큰 처리 task들에 finetuning 방식을 적용할 때, 매우 좋지 않다.

- In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. 
	- BERT는 단방향 학습을 극복하기 위해 "masked language model"(MLM)을 사용했다.
	- MLM은 랜덤하게 토큰을 마스킹한 후, 마스킹 된 토큰을 맞추도록 학습시키는 것이다. MLM은 토큰의 순서와 관계 없는 학습이기 때문에 결과적으로 양방향 학습이 된다.
- In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows: 
	- 또한 "next sentence prediction"(NSP) task를 사용했다.
	- NSP는 문장 2개를 짝지운 후, 하나의 문장을 랜덤하게 마스크하고 다음 문장을 맞추도록 학습시키는 것이다.
- We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs.
	- BERT는 MSM을 통해 깊은 양방향 학습을 한다. 이는 얕은 양방향 학습(왼쪽 → 오른쪽으로 학습 시킨 후, 다시 오른쪽 → 왼쪽으로 학습시킴)과는 다르다
- We show that pre-trained representations reduce the need for many heavily-engineered task specific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-specific architectures. 
	- 사전 학습을 이용하면, 특정 task를 위해 많은 노력을 들여 아키텍쳐를 설계할 필요가 없다.
	- BERT는 최초의 finetuning 기반의 사전 학습 모델이며, 특정 task를 위해 만들어진 아키텍쳐들의 성능을 능가한다.
- BERT advances the state of the art for eleven NLP tasks. The code and pre-trained models are available at <https://github.com/google-research/bert>.
	- 코드는 여기에

## Related Work
- There is a long history of pre-training general language representations, and we briefly review the
most widely-used approaches in this section.
	- 사전 학습을 활용한 언어 표현에는 유구한 역사가 있는데, 잠깐 리뷰하겠다.

### Unsupervised Feature-based Approaches
- Learning widely applicable representations of words has been an active area of research for decades, including non-neural (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) and neural (Mikolov et al., 2013; Pennington et al., 2014) methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering significant improvements over embeddings learned from scratch (Turian et al., 2010).
 	- 사전 학습된 단어 임베딩을 사용하면 task 성능이 비약적으로 높아진다.
- To pretrain word embedding vectors, left-to-right language modeling objectives have been used (Mnih and Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and right context (Mikolov et al., 2013)
	- 단어 임베딩 벡터를 사전 학습시키기 위해, 단어의 바로 다음(오른쪽)에 출현하는 단어가 적절한지 판단하는 방식을 사용했다.
- These approaches have been generalized to coarser granularities, such as sentence embeddings (Kiros et al., 2015; Logeswaran and Lee, 2018) or paragraph embeddings (Le and Mikolov, 2014). To train sentence representations, prior work has used objectives to rank candidate next sentences (Jernite et al., 2017; Logeswaran and Lee, 2018), left-to-right generation of next sentence words given a representation of the previous sentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).
	- 이런 접근 방식은 문장 임베딩, 문단 임베딩에도 응용되었다.
	- 문장 표현을 사전 학습하기 위해, 다음에 출현할 문장 후보들의 랭킹 매기기, 현재 문장 표현을 주고 다음에 출현할 문장의 단어를 왼쪽 → 오른쪽 방향으로 생성하기, 오토 인코더의 노이즈 제거하기 등의 방법을 사용했었다.
- ELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks (Peters et al., 2018a) including question answering (Rajpurkar et al., 2016), sentiment analysis (Socher et al., 2013), and named entity recognition (Tjong Kim Sang and De Meulder, 2003).
	- ELMo는 왼쪽 → 오른쪽 방향의 언어 모델과 오른쪽 → 왼쪽 방향의 언어 모델을 조합해서 문맥을 잘 표현할 수 있는 단어 임베딩을 선보였고, state-of-the-art를 달성했다. 이는 얕은 양방향 학습이다.
- Melamud et al. (2016) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional.
	- LSTM 기반, 양방향 단어를 맞추는 작업을 통해 문맥을 표현하는 방식도 제안되었는데, 이는 ELMo와 비슷한 개념이며, 역시 얕은 양방향 학습이다.
- Fedus et al. (2018) shows that the cloze task can be used to improve the robustness of text generation models. 
	- cloze task(문장 내 제거된 단어를 맞추는 task)이 문장 생성 모델의 신뢰도를 높여준다.

### Unsupervised Fine-tuning Approaches
- As with the feature-based approaches, the first works in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008).
	- 처음에는 fine-tuning 접근법에서도 feature-based 접근법 처럼 단어 임베딩 파라미터들만 사전 학습되었다.
- More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018).
	- 이후 문맥을 표현하는 토큰들을 출력하는 문장 및 문서 인코더들이 사전 학습 되었고, 특정 목적을 가진 지도 학습 task를 위해 fine-tuning 되었다.
- The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved previously state-of-the-art results on many sentence level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language modeling and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). 
	- 이 방식의 장점은 아예 처음부터 학습해야하는 파라미터가 거의 없다는 것이다.(학습된 파라미터들을 이미 다 가지고 있다.) OpenAI GPT는 이런 장점을 활용해서 많은 문장 레벨 NLP 작업들에서 state-of-the-art를 달성했다.
	- 사전 학습을 위해 텍스트를 왼쪽 → 오른쪽 방향으로 학습한 언어 모델과 오토 인코더 등이 사용되었다.

![](/media/posts/bert/pre_training_vs_fine_tuning.PNG)

- Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating questions/answers).
	- 출력 레이어를 제외하면 pre-training과 fine-tuining의 아키텍쳐는 같다.
	- 다른 목적을 가진 여러 task들(MNLI, NER, SQuAD, ...)에 같은 pre-training 모델 파라미터들을 사용할 수 있다.
	- fine-tuning 중에 모든 파라미터들이 미세 조정된다.

### Transfer Learning from Supervised Data
- There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference (Conneau et al.,2017) and machine translation (McCann et al.,2017). Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).
	- 자연어 추론, 기계 번역 등, 큰 데이터 셋을 사용한 지도 학습으로부터 전이 학습의 효과를 확인했다.
	- 사전 학습된 큰 모델로부터의 전이 학습이 중요하다는 것이 컴퓨터 비전 리서치에서도 증명되었다.

## BERT
- We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For finetuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its unified architecture across different tasks. There is minimal difference between the pre-trained architecture and the final downstream architecture. 
	- 사전 학습(레이블링 되지 않은 데이터로 학습) + 미세 조정(task의 목적에 맞게 레이블링 된 데이터로 학습)
	- 사전 학습시 학습했던 파라미터 중, 미세 조정 task에 필요한 것들만 뽑아서 미세 조정 과정에서 추가 학습 시킴
	- 사전 학습에 포함된 파라미터들은 여러 미세 조정 task에 대응할 수 있게 설계되었다.

### Model Architecture
- BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. (2017) as well as excellent guides such as “The Annotated Transformer.”2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A. 3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M). BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left. 
	- BERT는 트랜스포머의 인코더 부분을 차용했다. [Transformers 인코더 구조 참조](http://chorockuin.github.io/2020/04/11/transformers.html)
	- L
		- 레이어 수(트랜스포머 블럭의 수)
	- H
		- 히든 사이즈(토큰 벡터의 표현 해상도)
	- A
		- 어텐션 헤드 수(트랜스포머의 셀프 어텐션 헤드 수)
	- BERTBASE의 모델 크기
		- L=12, H=768, A=12, 총 파라미터 수=110M
		- 비교를 위해 OpenAI GPT의 모델 크기에 맞췄다.
	- BERTLARGE의 모델 크기
		- L=24, H=1024, A=16, 총 파라미터 수=340M

### Input/Output Representations
- To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answeri) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together. We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ([SEP]). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the final hidden vector of the special [CLS] token as $$C \in R^{H}$$, and the final hidden vector for the $$i^{th}$$ input token as $$T_i \in R^{H}$$. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2 
	- 입력 데이터 형식은 "[CLS]첫번째 시퀀스 토큰들[SEP]두번째 시퀀스 토큰들" 이다.
	- 시퀀스는 단일 문장이 될 수도 있고, 복수 문장이 될 수도 있다. 시퀀스는 토큰들로 표현한다.
	- 시퀀스를 토큰으로 표현할 때, 약 30,000개의 토큰 어휘를 갖는 WordPiece 토크나이저를 사용한다.
	- [CLS]는 하나의 입력 데이터가 시작됨을 나타내는 스페셜 토큰이다. 하나의 입력 데이터를 대표하므로, 이 토큰의 마지막 히든 벡터를 시퀀스 분류 fine-tuning task의 입력으로 사용할 수 있다.
	- [SEP]는 하나의 입력 데이터 내에서 두 개의 시퀀스를 구분해야 하는 task(Q&A task 등)에서 두 시퀀스를 구분하기 위해 사용하는 스페셜 토큰이다. 따라서 task에 따라 필요할 수도 있고 그렇지 않을 수도 있다.
	- 입력 토큰의 임베딩 = 토큰 ID(0 ~ 약 30,000) + 토큰의 시퀀스 ID(0 ~ 1) + 시퀀스 내 토큰 위치(0 ~ 511)

![](/media/posts/bert/input_representation.PNG)

### Pre-training BERT
#### Task #1: Masked LM
#### Task #2: Next Sentence Prediction (NSP)
#### Pre-training data
### Fine-tuning BERT

## Experiments
### GLUE
### SQuAD v1.1
### SQuAD v2.0
### SWAG

## Ablation Studies
### Effect of Pre-training Tasks
### Effect of Model Size
### Feature-based Approach with BERT

## Conclusion
