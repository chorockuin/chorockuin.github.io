---
layout: post
title: "NLP Basic"
date: 2020-03-28
<!--banner_image: .jpg-->
tags: [AI, DL, NLP]
---
**자연어**
- 사람이 일상적으로 쓰는 언어

**단어**
- 의미의 최소 단위는 형태소
- 형태소의 집합이 단어

**토큰**
- NLP에서 처리 단위
- 단어 단위, 형태소 단위, 글자 단위 등 정하기 나름
- 수행하고자 하는 NLP 작업(task)의 목적에 따라 정함
- 보통 영어에서는 단어, 한글에서는 형태소가 토큰의 단위가 됨

**Word Piece**
- 토크나이저
- 단어 단위로 토크나이징하면 토큰 수가 너무 많음
- 문장에 자주 출현하는 단어의 조각은 분리되지 않고 독립된 의미를 갖게될 것이라는 가정을 이용
	- I'm a scientist and artist 문장에서 tist는 독립된 의미를 갖을 것이며, 공유될 것이다
- 많은 문장을 학습하여 독립된 의미를 갖는 단어 조각들의 맵을 생성
- 새로운 문장이 들어오면, 생성해 둔 단어 조각들의 맵에 따라 토크나이징
- '자주 출현하는 단어의 조각'이란 개념은 언어의 종류에 독립적이므로 모든 언어에서 사용 가능
- <https://lovit.github.io/nlp/2018/04/02/wpm/>

**그램**
- 문맥의 의미를 알기 위해 사용한 토큰의 갯수(bigram, trigram, n-gram)
	- 토큰 2개를 사용해서 문맥의 의미를 파악하려고 했다면 bigram
	- NLP is fun and NLP is not boring
		- 단어를 토큰으로 사용했다고 했을 때,   
		  1개의 그램으로는 NLP가 재미있는지 지루한지 파악 못함   
		  2개의 그램을 사용해야 is fun과 not boring을 구분할 수 있음

**단어 표현 법**
- 단어를 고정길이 벡터로 표현
- 시소러스, 통계 기반, 추론 기반

**시소러스**
- 유의어 그룹화/계층화 사전
	- SUV, 승용차, 트럭 - 자동차 - 탈것 - 물체
- WordNet(NLTK)이 유명
- 수작업, 신조어 대응 취약, 뉘앙스 표현 불가

**통계 기반**
- 분포 가설 이용
	- 단어의 의미는 맥락(주변 단어)에 의해 형성
- 단어를 단어 주변(앞/뒤)에 출현하는 단어의 벡터로 표현(분산 표현)
	- you say goodbye and i say hello
		- you : say[0, 1, 0, 0, 0, 0]
		- say : you, goodbye, i, hello[1, 0, 1, 0, 1, 1]
- 대량의 텍스트 데이터(Corpus)를 이용해 단어들의 분산 표현(벡터)을 얻음
- 단어 간의 연관도는 분산 표현된 단어 벡터 간의 코사인 유도로 구함
- 단순히 앞/뒤에 출현하는 단어라고 해서 연관 있는 단어는 아님
	- I drive the car에서 drive는 the가 아닌, car와 더 연관됨
	- 동시 출현(the car) 외에 독립 출현(the, car 각각)도 고려해서 연관도를 구해야 함(PMI)
	- PMI = the car 동시 출현 횟수 / (the 독립 출현 횟수 * car 독립 출현 횟수)
- 단어의 수가 많아지면 분산 표현된 단어 벡터의 차원수가 증가함
	- SVD를 사용해 차원을 축소
	- 단어의 수가 100만을 넘기면 계산 불가
- 텍스트 데이터 배치 처리 불가, 확장/변경에 취약
- 단어 간 유사성만 표현됨

**추론 기반(word2vec)**
- 통계와 동일하게 분포 가설 이용
- 주변(앞/뒤) 단어를 주고, 해당 단어를 추론하도록 학습(CBOW)
	- you say goodbye and i say hello
		- you, goodbye : say로 모델 학습
- 학습 후, 주변(앞/뒤) 단어를 모델에 통과시키면, 각 단어들이 출현할 확률을 나타내는 벡터를 얻을 수 있음
	- 입력 : you, goodbye   
	  출력 : you(0.05), say(0.65), goodbye(0.05), and(0.1), i(0.1), hello(0.05)   
- 학습 후, 얻은 입력 가중치 행렬이 단어들의 분산 표현
	- 입력층 값 : [1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]   
	  입력 가중치 W_in : [a1, x, b1, x, x, x], [a2, x, b2, x, x, x], [a3, x, b3, x, x, x]   
	  은닉층(뉴런 3개 가정) 값 h : [a1 + b1, a2 + b2, a3 + b3]   
	  출력 가중치 W_out : [x, x, x], [x, x, x], [x, x, x], [x, x, x], [x, x, x], [x, x, x]   
	  출력층 값 : [0.05, 0.65, 0.05, 0.1, 0.1, 0.05]
	  학습시킬 때 값 : [0, 1, 0, 0, 0, 0]   
	  you의 분산 표현 : [a1, a2, a3]   
	  goodbye의 분산 표현 : [b1, b2, b3]	  
- 단어를 주고 주변(앞/뒤) 단어를 추론하도록 학습시킬 수도 있음(skip-gram)
- 일반적으로 skip-gram이 CBOW보다 성능이 더 좋음
- 텍스트 데이터 미니배치 처리 가능, 확장/변경에 유연
- 단어 간 유사성을 너머 복잡한 관계도 표현 가능
	- king - man + woman = queen과 같은 유추 문제 해결 가능

**추론 기반 + 통계 기반**
- Glove
	- 통계 기반 정보를 손실 함수에 도입해 미니배치 학습

**문장 표현 법**
- 문장을 고정길이 벡터로 표현하여 전이학습(분류, Seq2Seq 등)의 데이터로 사용
- bag of words
	- 문장을 구성하는 각 단어(word2vec)의 합으로 표현
	- 문장 = 단어의 합 = 단어들이 순서없이 섞여있는 가방
	- 단어의 순서가 고려되지 않아 문장 표현으로 부적절
- 언어모델
	- 단어의 순서가 고려된 언어모델을 만들면, 고정길이 벡터로 표현된 문장을 얻을 수 있음

**언어모델**
- 순서대로 나열된 단어들(문장) 뒤에 이어질 단어를 주어주고, 해당 단어를 추론하도록 학습
- 학습 후, 순서대로 나열된 단어들(문장)를 모델에 통과시키면, 이어질 단어들의 출현 확률을 나타내는 벡터를 얻을 수 있음
	- you say 다음에 나올 단어의 확률 : goodbye(0.35), and(0.3), i(0.05), hello(0.3)
- 성능은 퍼플렉서티(perplexity)로 평가
	- 문장 뒤에 이어질 수 있는 단어의 후보 수
- 확률에 기반해 문장 뒤에 이어질 자연스러운 단어를 추론할 수 있음(문장 생성 가능)
	- 언어 모델 : you say goodbye(0.99), you say and(0.02)
- word2vec과는 달리 순서가 고려됨
	- 언어 모델 : you say(0.99), say you(0.02)   
	- word2vec : you say(0.99), say you(0.99)
- 데이터의 순서가 반영되는 RNN을 사용하여 언어모델을 만들 수 있음

**RNN**
- 이전 데이터의 신경망 처리 출력값/가중치를 다음 데이터의 신경망 처리 입력으로 사용
	- you say good bye and i say hello
		- you → RNN → you의 가중치(분산 표현), you의 출력
		- say, you의 가중치, you의 출력 → RNN → say의 가중치, say의 출력
		- say의 가중치, say의 출력에는 you의 특징(you의 가중치, you의 출력)이 반영됨
- 이전 데이터의 특징이 반영되므로 순서가 고려됨
- 순서가 고려되어야 하는 시계열 데이터(문장, 음성 등)의 신경망 처리에 사용

**seq2seq**
- 순서가 있는(시계열/시퀀스) 데이터를 다른 형태의 순서가 있는(시계열/시퀀스) 데이터로 변환
	- 번역 : 한국어 문장 → 영어 문장
	- 음성인식 : 음성 → 문장
	- 챗봇 : 문장 → 문장
- 데이터 → 인코더 → 인코딩된 데이터 → 디코더 → 원하는 데이터
- LSTM을 이용한 한국어 문장 → 영어 문장 번역
	- 인코더 및 디코더를 LSTM으로 구성
	- 한국어 문장을 인코더에 입력, 문장을 표현하는 인코더 은닉층의 고정길이 벡터 h를 출력으로 얻음
	- 고정길이 벡터 h를 디코더의 첫번째 입력으로 사용, 이후엔 영어 문장을 디코더에 입력
	- 영어 문장을 출력으로 설정하여 학습시킴
		- 인코더 입력 : '넌 잘가라고 말하고, 난 안녕이라고 말한다'의 임베딩 [x, x, x, x, x, x, x]   
		  인코더 출력 : [x, x, x]   
		  출력 학습 데이터 : you say goodbye and i say hello   
		  디코더 첫번째 입력 : [x, x, x] + EOS(end of sentence) 임베딩 [x, x, x, x, x, x]   
		  디코더 첫번째 LSTM의 출력 : you의 임베딩 [x, x, x, x, x, x]   
		  디코더 두번째 입력 : 디코더의 첫번째 LSTM 출력 + you의 임베딩 [x, x, x, x, x, x]   
		  디코더 두번째 LSTM의 출력 : say의 임베딩 [x, x, x, x, x, x]   
		
- peeky 디코더
	- 인코더의 출력인 고정길이 벡터 h를 디코더의 각 시계열 데이터에 모두 사용
	- h를 디코더의 첫번째 시계열 데이터에만 사용하는 것보다 성능 좋음

**어텐션**
- LSTM 인코더의 문제점
	- 문장이 길면 고정길이 벡터 h의 문장 표현력이 떨어짐
	- 입력-출력 시계열 데이터들 간 대응관계가 있음에도 불구하고, h가 출력 시계열 데이터 모두에게 동일하게 영향을 미침
- 문장의 단어가 다른 문장의 어떤 단어에 '주목'하는지 벡터로 표현
	- you say goodbye and i say hello
		- you : say, goodbye, and, i, hello[0.4, 0.2, 0.1, 0.2, 0.1]
- 셀프 어텐션
	- 문장의 단어가 같은 문장 내에 어떤 단어에 '주목'하는지 벡터로 표현
	- 현재 단어(Query)와 문장 내 단어들(Key)의 내적을 구하고 softmax 후, 문장 내 단어들(Value)과 다시 곱함

**트랜스포머**
- <https://www.youtube.com/watch?v=mxGCEWOxfe8&t=581s>

**NLP 라이브러리**
- [Huggingface(Tensorflow + Pytorch 기반)](https://github.com/huggingface/transformers/tree/v2.6.0#installation)
- [Google(Tensorflow 기반)](https://github.com/tensorflow/models/tree/master/official/nlp)
- [GluonNLP(MXNet 기반)](https://gluon-nlp.mxnet.io/)

**데이터 셋**
- IMDb : 영화리뷰데이터 셋(리뷰텍스트+긍정/부정)
- GLUE의 DataSet
	- MNLI(Multi-Genre Natural Language Inference): entailment classification task
	- QQP(Quora Question Pairs): Quora에 올라온 질문 페어가 의미적으로 동일한지 확인하는 테스크
	- QNLI(Question Natural Language Inference): SQuAD의 이진분류 버전. paragraph가 answer를 포함하는지 안하는지 확인하는 문제.
	- SST-2(Stanford Sentiment Treebank): 단문장 이진분류문제. 영화리뷰에서 추출된 문장에 감정이 표기되어있음.
	- CoLA(Corpus of Linguistic Acceptability): 영어문장이 언어학적으로 acceptable한지 확인하는 이진분류문제
	- STS-B(Seemantic Textual Similarity Benchmark): 문장쌍이 얼마나 유사한지 확인하는 문제.
	- MRPC(Microsoft Research Paraphrate Corpus): 문장쌍의 유사성 확인하는 문제.
	- RTE(Recognizing Textual Entailment): MNLI와 유사하나 데이터가 적음.
	- WNLI(Winograd NLI): 자연어 추론 데이터셋이나 현재 채점에 이슈가 있어서 BERT 실험에서는 제외됨.

**상식**
- .tsv 파일
	- tab으로 구분된 데이터 형식을 갖는 파일
- NCCL(NVIDIA Collective Communication Library)
	- NVIDIA 칩을 사용해, 분산 학습 시, 분산된 모듈 사이 통신을 담당하는 라이브러리
- from scratch
	- 바닥부터 학습을 시작
- from checkpoint
	- 저장된 가중치 값들(checkpoint)부터 학습을 시작
- zero shot
	- 테스트 데이터를 학습에 조금도 이용하지 않고, 테스트에만 이용