---
layout: post
title: "DL 기초"
date: 2020-04-11
<!--banner_image: .jpg-->
tags: [AI, DL]
---
**CUDA(Computed Unified Device Architecture)**
- NVIDIA GPU 프로그래밍을 쉽게 할 수 있도록 도와주는 C기반 라이브러리
- CPU는 직렬처리, GPU는 병렬처리를 잘함(https://www.youtube.com/watch?v=-P28LKWTzrI#action=share)
- DL은 동시에 많은 행렬 계산을 요구하므로, GPU를 사용함

**OpenCL**
- CUDA와 같은 GPU 프로그래밍 라이브러리.
- 애플, 인텔, AMD, ARM등 에서는 NVIDIA를 견제하기 위해 이 녀석을 밀고 있음

**Learning Rate(학습률)**
- 얼마의 비율로 학습할 것인가?를 나타내는 값
- 매 학습이 끝날 때마다 가중치 갱신 값이 나오는데, 이 값에 학습률을 곱해서 실제 가중치 갱신에 사용

**엔트로피**
- 불확실성을 나타냄
	- 불확실성 ↑ 엔트로피 ↑ 
	- 불확실성 ↓ 엔트로피 ↓
- 정보량
	- 정보량 = 알고있는 정보량 + 모르는 정보량
	- 모르는 정보량 ↑ 불확실성 ↑ 엔트로피 ↑
	- 모르는 정보량 ↓ 불확실성 ↓ 엔트로피 ↓
- 정보 발생 확률과 불확실성
	- 정보 발생 확률 ↑ 모르는 정보량 ↓ 불확실성 ↓ 엔트로피 ↓
	- 정보 발생 확률 ↓ 모르는 정보량 ↑ 불확실성 ↑ 엔트로피 ↑
- 엔트로피는 정보량에 비례, 정보 발생 확률에 반비례
- 섀넌 엔트로피 공식으로 엔트로피 계산
	- 모든 정보(알고 있는 정보 + 모르는 정보)별 (정보량 x 정보 발생 확률)의 합
	- 기대값과 비슷한 개념
		- 모든 사건별 (사건 값 x 사건 발생 확률)의 합
		- 주사위를 1번 던졌을 때의 기대값은 (1/6 + 2/6 + 3/6 + ...) = 3.5
	- 정보량과 정보 발생 확률은 반비례 하므로 정보량을 정보 발생 확률의 역으로 표현
	- 밑이 2인 로그를 쓰는 것은 bit를 표현하기 위함
		- 정보량이 16이라고 한다면, 엔트로피는 4가 됨
		- 4bit로 최대 16개의 정보를 나타냄

**크로스 엔트로피**
- 섀넌 엔트로피 공식에서 정보량 계산 부분만 다름
- 정보량 계산에 실제 정보 발생 확률값이 아닌, 모델을 통해 예측한 정보 발생 확률값을 사용
- 실제 정보 발생 확률 값을 사용하여 구한 엔트로피 값과의 차이를 표현할 수 있음   
  동시에 엔트로피의 원 개념, 즉 불확실성도 표현할 수 있음
- 크로스 엔트로피 값 >= 엔트로피 값
- DL에서는 크로스 엔트로피 값이 최소가 되도록 모델을 학습시킴
	- 실제 정보 발생 확률 값 : 1.0   
	  모델을 통해 예측한 정보 발생 확률 값 : 0.0, 크로스 엔트로피 값 : 무한대   
	  모델을 통해 예측한 정보 발생 확률 값 : 1.0, 크로스 엔트로피 값 : 0.0

**Precision(정밀도)**
- 실제 데이터의 비율은 얼마나 되는가?
- 모델이 True라고 예측한 데이터의 수와 실제 True인 데이터 수의 비율
- 100개의 데이터 중에 50개가 True인데, 모델이 80개를 True라고 예측했고 그 중 40개가 True 였다면,   
  Precision = 40/80 = 0.5   
  Recall = 40/50 = 0.8

**Recall(재현율)**
- 실제 데이터를 얼마나 소환했는가?
- 실제 True인 데이터 수와 모델이 True라고 예측한 데이터 수의 비율
- 100개의 데이터 중에 50개가 True인데, 모델이 2개를 True라고 예측했고 그 중 2개 모두 True 였다면,   
  Precision = 2/2 = 1.0   
  Recall = 2/50 = 0.04
  