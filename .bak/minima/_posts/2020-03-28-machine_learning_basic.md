---
layout: post
title: "Machine Learning Basic"
---
## 명시적 프로그래밍 vs 머신러닝 vs 딥러닝
- 목표에는 차이가 없다.
	- y = f(x)
	- 알고리즘 f에 데이터 x를 입력하면 y로 결과를 예측한다.
		- *키(x)를 입력하면 몸무게(y)를 예측한다.*
- 알고리즘 f를 만들어내는 방식의 차이다.
	- 명시적 프로그래밍
		- 사람이 입력과 출력의 관계를 따져서 명시적인 로직을 세운다.
		- 명시적인 로직이 곧 알고리즘이 된다.
			- *몸무게 = 키 - 110*
	- 머신러닝
		- 명시적 로직이 아닌, 사람이 어느정도 추상화시킨 로직들을 세우고, 컴퓨터는 로직들이 최적의 속성 값을 가지도록 입/출력 데이터를 통해 학습한다.
		- 적절한 속성 값을 가진 로직들이 알고리즘이 된다.
			- *몸무게 = a x 키 + b*
			- *로직에 키와 몸무게 데이터들을 넣어 속성 값 a, b를 찾아낸다.*
	- 딥러닝
		- 사람이 로직을 세우지 않고, 신경망이 로직을 대체한다. 컴퓨터는 신경망이 최적의 파라미터 값을 가지도록 입/출력 데이터를 통해 학습한다.
		- 적절한 파라미터 값을 가진 신경망이 알고리즘이 된다.
			- *a, b, c, ... 등의 파라미터를 가진 신경망에 키를 입력하여 몸무게를 추론*
			- *신경망에 다량의 키 데이터를 입력하고, 결과 값인 몸무게 데이터와 비교하여 신경망의 파라미터 값 a, b, c, ...를 찾아낸다.*

## 엔트로피(Entropy)
- 엔트로피는 불확실성을 나타낸다.
	- 불확실성 ↑ 엔트로피 ↑ 
	- 불확실성 ↓ 엔트로피 ↓
- 확률로 결정되는 정보가 있는데, 정보량이 많아지면 불확실성이 증가한다. 엔트로피가 증가한다.
	- *동전을 던져 앞면이 나올 경우를 예측해야 하는 상황일 때, 동전이 2개일 때보다 100개일 때 불확실성이 더 높다*
	- 정보량 ↑ 불확실성 ↑ 엔트로피 ↑
	- 정보량 ↓ 불확실성 ↓ 엔트로피 ↓
- 확률로 결정되는 정보가 있는데, 확률이 높아지면 정보량이 줄어드는 셈이다. 정보량이 줄어들면 불확실성이 낮아져 엔트로피가 감소한다.
	- *동전을 던져 앞면이 나올 경우를 예측해야 하는 상황일 때, 동전의 앞면이 나올 확률이 50%일 때보다 90%일 때 불확실성이 더 낮다*
  - 정보량과 정보 발생 확률은 반비례 관계이다.
  - 정보 발생 확률 ↑ 정보량 ↓ 불확실성 ↓ 엔트로피 ↓
	- 정보 발생 확률 ↓ 정보량 ↑ 불확실성 ↑ 엔트로피 ↑
- 정보 종류 별로 정보량과 정보 발생 확률을 곱한 후 모두 더하면 엔트로피를 구할 수 있다.
	- 사건 별로 사건값과 사건 발생 확률을 곱한 후 모두 더해서 구하는 기대값과 비슷한 개념이다.
	- *주사위를 1번 던졌을 때의 기대값은 $${1 \over 6} + {2 \over 6} + {3 \over 6} + ... = 3.5$$*

## 섀넌 엔트로피(Shannon Entropy)

$$
H(x) = -\sum_{i=1}^n p_i\log_{2} p_i
$$
 
- 엔트로피 = 정보별 (정보량 x 정보 발생 확률)의 합, 그리고 정보량과 정보 발생 확률은 반비례 관계라는 개념을 이용한 공식이다.
- 아래는 같은 표현이다.

$$
H(x) = \sum_{i=1}^n p_i\log_{2} {1 \over p_i}
$$

- $$p_i$$는 정보 발생 확률, $$\log_{2} {1 \over p_i}$$는 정보량을 나타낸다. 정보량과 정보 발생 확률은 반비례하므로, 정보량을 정보 발생 확률의 역($${1 \over p_i}$$)으로 표현했다.
- 정보량 표현에 밑이 2인 로그를 쓰는 것은 비트를 표현하기 위한 것이다.
	- *16개의 정보를 표현하기 위해서는 4개의 비트가 필요하다. 엔트로피 관점에서 보면, 4라는 불확실성은 16개라는 정보량을 표현할 수 있다.*

## 크로스 엔트로피(Cross Entropy)

$$
H(P,Q) = \sum_{i=1}^n p_i\log_{2} {1 \over q_i}
$$

- 섀넌 엔트로피 공식에서 정보량 계산 부분만 다르다. $$p_i$$가 $$q_i$$로 바뀌었다.
- 정보량 계산에 실제 정보 발생 확률값 $$p_i$$가 아닌, 모델을 통해 예측한 정보 발생 확률값 $$q_i$$를 사용했다.
- 실제 정보 발생 확률 값을 사용해 구한 엔트로피 값과의 차이를 표현할 수 있다.
- 동시에 엔트로피의 원 개념, 즉 불확실성도 표현할 수 있다.
- 크로스 엔트로피 값 >= 엔트로피 값
- 머신러닝에서는 크로스 엔트로피 값이 최소가 되도록 모델을 학습시킨다.
	- *실제 정보 발생 확률 값 $$p_i$$가 1.0일 경우*
	- *모델을 통해 예측한 정보 발생 확률 값 $$q_i$$가 0.0이라면 크로스 엔트로피 값은 무한대*
	- *모델을 통해 예측한 정보 발생 확률 값 $$q_i$$가 1.0이라면 크로스 엔트로피 값은 0.0*

## 정확도(Accuracy)
- 얼마나 정확히 예측했나?
- (예측한 정답 중에 실제 정답 수(TP) + 예측한 오답 중에 실제 오답 수(TN)) / 전체 수(TP+TN+FP+FN)
- *100개의 데이터 중에 50개가 정답인데, 25개를 정답으로 예측했고 75개를 오답으로 예측했다. 정답으로 예측한 25개 중 10개만 실제 정답이었다면,*
	- *정답으로 예측한 것 중에 정답(TP) = 10개*
  - *정답으로 예측한 것 중에 오답(FP) = 15개*
  - *오답으로 예측한 것 중에 정답(FN) = 50-10=40개*
  - *오답으로 예측한 것 중에 오답(TN) = 75-40=35개*
	- *정확도 = (TP+TN)/(TP+TN+FP+FN) = (10+35)/100 = 0.45*

## 정밀도(Precision)
- 얼마나 정밀하게 정답을 예측했나? 예측한 정답 중에 실제 정답의 비율은 얼마나 되는가?
- 모델이 정답이라고 예측한 데이터 중에 실제 정답 데이터의 비율
- TP / (TP + FP)
- 위의 예에서 보면
	- *정밀도 = 10/(10+15) = 0.4*

## 재현률(Recall)
- 얼마나 정답을 재현했나? 실제 정답 중 재현된 정답의 비율은 얼마나 되는가?
- 실제 정답인 데이터 중에 모델이 정답이라고 예측한 데이터의 비율
- TP / (TP + FN)
- 위의 예에서 보면
	- *정밀도 = 10/(10+40) = 0.2*

## F1-점수(F1-Score)
- 정밀도와 재현률의 조화평균(역수의 산술평균 값의 역수)
- 편향을 고려해 평균을 표현할 수 있다.
- 정밀도가 0.4, 재현률이 0.2라고 한다면,
	- *F1-Score = 2 x (0.4 x 0.2) / (0.4 + 0.2) = 0.26666*

## 사이킷런의 함수들
- from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
- 파라미터 중 average의 의미
	- binary
		- 이진 분류에서 특정 label(default=1)에 대해서만 계산함. 보통 label 1은 정답을 의미하므로, 정답에 대해 계산하는 셈
	- micro
		- 다중 분류에서 각 label에 대해 각각 계산하지 않고, 모두 하나로 계산함
	- macro
		- 다중 분류에서 각 label에 대해 각각 계산한 후, 평균을 구함
	- weighted
		- 다중 분류에서 각 label에 대해 각각 계산한 후, 가중 평균을 구함. 각 label의 데이터 개수가 다를 경우를 감안

## 오즈(Odds)
- 성공비율/실패비율, 
- 성공 비율을 p라고 한다면, 

$$
O = {p \over 1-p}
$$

- *10번 중 3번 성공한다면, 성공비율은 0.3, 실패비율은 0.7. 따라서 odds = 0.3/0.7 = 0.428*

## 로짓(Logit)
- logit = log + odds. 즉, odds에 자연로그를 씌운 것

$$
L = ln{p \over 1-p}
$$

- 역함수를 구한 후, $$e^{-L}$$를 곱해주면 시그모이드 함수가 된다.

$$
p = {e^{-L} \over e^{-L}+1}
$$

- 시그모이드 함수 값(확률)의 범위는 0.0 ~ 1.0, 로짓의 범위는 -$$\infty$$ ~ $$\infty$$
- 위와 같은 이유로, 확률화 시키지 않은 날것의 예측결과를 로짓이라고 부름. 보통 softmax 함수의 입력으로 쓰인다.

## 학습률(Learning Rate)
- 얼마의 비율로 학습할 것인가?를 나타내는 값
- 매 학습이 끝날 때마다 가중치 갱신 값이 나오는데, 이 값에 학습률을 곱해서 실제 가중치 갱신에 사용한다.

## 참조
- <https://www.youtube.com/watch?v=CdH7U3IjRI8>
